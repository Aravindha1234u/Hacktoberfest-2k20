# -*- coding: utf-8 -*-
"""housing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BvCW9W9yc4KFOVwhi7vCWo5qG2HQPOUM
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import scale
from sklearn.metrics import mean_squared_error, r2_score

# loading data
data = load_boston()
df = pd.DataFrame(np.c_[data.data,data.target], columns= np.append(data.feature_names,["MEDV"]))

df.head()

# Check for missing values
df.isnull().sum()

# Check distribution of each variable
pos = 1
fig = plt.figure(figsize=(16,24))
for i in df.columns:
    ax = fig.add_subplot(7,2,pos)
    pos = pos + 1
    sns.histplot(df[i],ax=ax)

"""As we can see not all variables are normally distributed which is a problem. But our target variable is normally distributed."""

#check colinearity
sns.heatmap(df.corr())

"""From above heatmap we can see that our target variable is highly dependent on LSTAT and RM variables. But we can also see high mulitconlinearity among few variables. For e.g. RAD and TAX."""

X = df.iloc[:,0:13]
y = df.iloc[:,13]

# Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
scaler = StandardScaler()
# Fit only on train data
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# Principal Component Analysis
# PCA is used for removing multicolinearity from the data
pca = PCA(n_components=13)
pca.fit(X_train)
X_pca_train = pca.transform(X_train)
X_pca_test = pca.transform(X_test)

#variance ratio displays ratio of data represented by individual components
print(pca.explained_variance_ratio_)

"""Reason for fitting scaler and PCA only on Train data is because we should not let test data influence the Train data."""

#Lets see if PCA normalized the data
# Check distribution of each variable
pos = 1
fig = plt.figure(figsize=(16,24))
for i in range(13):
    ax = fig.add_subplot(7,2,pos)
    pos = pos + 1
    sns.histplot(X_pca_train[i],ax=ax)

# Lets see if PCA removed multicolinearity
pca_df = pd.DataFrame(np.c_[X_pca_train,y_train])
sns.heatmap(pca_df.corr())

"""PCA solved the problem of multicolinearity"""

# Model Fitting and Model prediction on Train data
lm = LinearRegression()
lm.fit(X_pca_train, y_train)
y_pred = lm.predict(X_pca_train)
print("rmse:",np.sqrt(mean_squared_error(y_train, y_pred)))
print("r2 score:",r2_score(y_train, y_pred))

# Model prediction on Test data
lm = LinearRegression()
lm.fit(X_pca_train, y_train)
y_pred = lm.predict(X_pca_test)
print("rmse:",np.sqrt(mean_squared_error(y_test, y_pred)))
print("r2 score:",r2_score(y_test, y_pred))